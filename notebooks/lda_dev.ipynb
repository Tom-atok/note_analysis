{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if 'root_dir' not in globals():\n",
    "    # rootディレクトリへのパスを設定\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "    os.chdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日時の取得\n",
    "from datetime import datetime\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# ロギングの設定\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#データベースへ接続するエンジンを作成\n",
    "from my_codes.database_setting import Engine\n",
    "from my_codes.database_setting import Base\n",
    "\n",
    "#データベースのテーブルとマッピングする\n",
    "from my_codes.notes_database import Notes\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import func\n",
    "\n",
    "#セッションを作成\n",
    "Session = sessionmaker(bind=Engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索語を指定して，特定の用語を含むノートを検索\n",
    "search_word = '農薬'\n",
    "result = session.query(Notes.key, Notes.created_at ,Notes.tokenized_body).filter(Notes.tokenized_body.like(f'%{search_word}%'))\n",
    "session.close()\n",
    "\n",
    "data = pd.DataFrame(result, columns=['key','create_at', 'tokenized_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # For converting string representation of lists to actual lists\n",
    "\n",
    "# Convert the string representation of lists in 'tokenized_body' to actual lists\n",
    "data['tokenized_body'] = data['tokenized_body'].apply(ast.literal_eval)\n",
    "\n",
    "# Display the transformed data to ensure correct conversion\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "# !pip install scipy==1.12\n",
    "# 最新のscipyのバージョンだとtriuがうまくダウンロードできないので、バージョンを指定してインストールする\n",
    "\n",
    "# Prepare the list of tokens for gensim\n",
    "texts = data['tokenized_body'].tolist()\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "# 各単語にユニークなIDを割り当てる\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert document into the bag-of-words (BoW) format = list of (token_id, token_count)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test_jupyter"
    ]
   },
   "outputs": [],
   "source": [
    "# Set up the LDA model\n",
    "num_topics = 10\n",
    "\n",
    "# LDAモデルを構築\n",
    "lda_model = models.LdaModel(\n",
    "    corpus=corpus,  # コーパス、文書のバグ・オブ・ワーズ表現\n",
    "    num_topics=num_topics,  # 抽出するトピックの数\n",
    "    id2word=dictionary,  # 単語IDと単語のマッピング\n",
    "    passes=3,  # トレーニング中にコーパス全体を繰り返す回数\n",
    "    iterations=50,  # 各パスで各文書内の反復回数\n",
    "    alpha='auto',  # トピック分布ごとのハイパーパラメータを自動で学習\n",
    "    eta='auto',  # 単語分布ごとのハイパーパラメータを自動で学習\n",
    "    random_state=42,  # 再現性のための乱数シード\n",
    "    chunksize=2000,  # 各トレーニングチャンクで使用される文書の数\n",
    "    update_every=5  # 各更新のために反復する文書の数\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the topics with their terms\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# 日本語フォントのパスを指定\n",
    "font_path = '/Library/Fonts/Arial Unicode.ttf'\n",
    "\n",
    "# Calculate number of rows and columns\n",
    "ncols = math.ceil(math.sqrt(num_topics))\n",
    "nrows = math.ceil(num_topics / ncols)\n",
    "\n",
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 16))\n",
    "\n",
    "for i, topic in enumerate(topics):\n",
    "    # トピック内の単語を連結\n",
    "    topic_words = ' '.join(topic[1].split('*'))\n",
    "    \n",
    "    # ワードクラウドを生成\n",
    "    wordcloud = WordCloud(font_path=font_path, width=400, height=200, colormap='Set2', background_color='white').generate(topic_words)\n",
    "    \n",
    "    # 該当するサブプロットにワードクラウドを描画\n",
    "    ax = axes[i // ncols, i % ncols]\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.set_title(f'Topic {topic[0]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "# 残りの空のサブプロットを非表示にする\n",
    "for j in range(i + 1, nrows * ncols):\n",
    "    fig.delaxes(axes[j // ncols, j % ncols])\n",
    "\n",
    "# サブプロット間の間隔を調整\n",
    "plt.tight_layout()\n",
    "\n",
    "# 画像を保存\n",
    "plt.savefig(f'img/word_cloud_{search_word}_{num_topics}_{now}.png')\n",
    "\n",
    "# ワードクラウドの結合画像を表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# 文書ごとのトピック分布を行列に変換\n",
    "topic_distributions = [lda_model[doc] for doc in corpus]\n",
    "topic_matrix = np.zeros((len(topic_distributions), num_topics))\n",
    "\n",
    "for i, dist in enumerate(topic_distributions):\n",
    "    for topic_id, prob in dist:\n",
    "        topic_matrix[i, topic_id] = prob\n",
    "\n",
    "# トピック間の相関行列を計算\n",
    "topic_correlation_matrix = np.corrcoef(topic_matrix.T)\n",
    "\n",
    "# DataFrameに変換して可視化\n",
    "topic_correlation_df = pd.DataFrame(topic_correlation_matrix)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(topic_correlation_df, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Topic Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドキュメントごとのトピック分布を取得\n",
    "doc_topic_dist = lda_model.get_document_topics(corpus, minimum_probability=0)\n",
    "\n",
    "# トピック分布をデータフレームに変換\n",
    "#doc_topic_dist = pd.DataFrame(doc_topic_dist)\n",
    "\n",
    "doc_topic_dist = [[topic_prob[1] for topic_prob in doc] for doc in doc_topic_dist]\n",
    "doc_topic_dist = pd.DataFrame(doc_topic_dist)\n",
    "\n",
    "# 文書全体におけるトピック分布の合計値を計算\n",
    "topic_dist_sum =doc_topic_dist.sum(axis=0)\n",
    "\n",
    "# 可視化\n",
    "topic_dist_sum.plot(kind='bar', figsize=(10, 6), color='skyblue')\n",
    "plt.savefig(f'img/topic_dist_sum_{search_word}_{now}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "\n",
    "# dataに重複する列があれば削除\n",
    "common_columns = data.columns.intersection(doc_topic_dist.columns)\n",
    "data = data.drop(columns=common_columns)\n",
    "\n",
    "# dataにdoc_topic_distを結合\n",
    "data = pd.concat([data, doc_topic_dist], axis=1)\n",
    "\n",
    "\n",
    "# 日付をインデックスに設定\n",
    "data['create_at'] = pd.to_datetime(data['create_at'])\n",
    "data.set_index('create_at', inplace=True)\n",
    "\n",
    "# 月ごとにトピック分布を集計\n",
    "monthly_topic_data = data.drop(columns=['key', 'tokenized_body']).resample('M').sum()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataをpickle形式で保存\n",
    "import pickle\n",
    "\n",
    "data.to_pickle(f'data/topic_dist_{search_word}_{now}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_topic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロット\n",
    "plt.figure(figsize=(12, 8))\n",
    "monthly_topic_data.plot(kind='bar', stacked=True, figsize=(12, 8))\n",
    "\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Topic Frequency')\n",
    "plt.title('Monthly Topic Distribution')\n",
    "plt.legend(title='Topics')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_topic_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# プロット用のカラーマップとスタイルを設定\n",
    "colors = plt.cm.tab20.colors\n",
    "styles = ['-', '--', ':', '-.']\n",
    "styles_extended = styles * (len(colors) // len(styles)) + styles[:len(colors) % len(styles)]\n",
    "color_cycler = cycler('color', colors)\n",
    "line_cycler = cycler('linestyle', styles_extended)\n",
    "combined_cycler = color_cycler + line_cycler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "ax = plt.gca()\n",
    "ax.set_prop_cycle(combined_cycler)\n",
    "\n",
    "# プロット\n",
    "for column in monthly_topic_data.columns:\n",
    "    plt.plot(monthly_topic_data.index, monthly_topic_data[column], label=column)\n",
    "\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Topic Frequency')\n",
    "plt.title(f'Monthly Topic Distribution {search_word}')\n",
    "plt.legend(title='Topics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'img/monthly_topic_distribution_{search_word}_{now}.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定のトピックの割合が高い文書を抽出，キーを取り出す．\n",
    "topic_id = 0\n",
    "top_n = 10\n",
    "top_docs = data.sort_values(by=topic_id, ascending=False).head(top_n)\n",
    "top_keys = top_docs['key'].tolist()\n",
    "\n",
    "top_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
