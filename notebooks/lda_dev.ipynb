{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if 'root_dir' not in globals():\n",
    "    # rootディレクトリへのパスを設定\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "    os.chdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# ロギングの設定\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#データベースへ接続するエンジンを作成\n",
    "from my_codes.database_setting import Engine\n",
    "from my_codes.database_setting import Base\n",
    "\n",
    "#データベースのテーブルとマッピングする\n",
    "from my_codes.notes_database import Notes\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import func\n",
    "\n",
    "#セッションを作成\n",
    "Session = sessionmaker(bind=Engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索語を指定して，特定の用語を含むノートを検索\n",
    "search_word = '料理'\n",
    "result = session.query(Notes.key, Notes.created_at ,Notes.tokenized_body).filter(Notes.tokenized_body.like(f'%{search_word}%'))\n",
    "session.close()\n",
    "\n",
    "data = pd.DataFrame(result, columns=['key','create_at', 'tokenized_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # For converting string representation of lists to actual lists\n",
    "\n",
    "# Convert the string representation of lists in 'tokenized_body' to actual lists\n",
    "data['tokenized_body'] = data['tokenized_body'].apply(ast.literal_eval)\n",
    "\n",
    "# Display the transformed data to ensure correct conversion\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "# !pip install scipy==1.12\n",
    "# 最新のscipyのバージョンだとtriuがうまくダウンロードできないので、バージョンを指定してインストールする\n",
    "\n",
    "# Prepare the list of tokens for gensim\n",
    "texts = data['tokenized_body'].tolist()\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "# 各単語にユニークなIDを割り当てる\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert document into the bag-of-words (BoW) format = list of (token_id, token_count)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LDA model\n",
    "num_topics = 20\n",
    "\n",
    "# LDAモデルを構築\n",
    "lda_model = models.LdaModel(\n",
    "    corpus=corpus,  # コーパス、文書のバグ・オブ・ワーズ表現\n",
    "    num_topics=num_topics,  # 抽出するトピックの数\n",
    "    id2word=dictionary,  # 単語IDと単語のマッピング\n",
    "    passes=3,  # トレーニング中にコーパス全体を繰り返す回数\n",
    "    iterations=50,  # 各パスで各文書内の反復回数\n",
    "    alpha='auto',  # トピック分布ごとのハイパーパラメータを自動で学習\n",
    "    eta='auto',  # 単語分布ごとのハイパーパラメータを自動で学習\n",
    "    random_state=42,  # 再現性のための乱数シード\n",
    "    chunksize=2000,  # 各トレーニングチャンクで使用される文書の数\n",
    "    update_every=5  # 各更新のために反復する文書の数\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the topics with their terms\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# 日本語フォントのパスを指定\n",
    "font_path = '/Library/Fonts/Arial Unicode.ttf'\n",
    "\n",
    "# Calculate number of rows and columns\n",
    "ncols = math.ceil(math.sqrt(num_topics))\n",
    "nrows = math.ceil(num_topics / ncols)\n",
    "\n",
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 16))\n",
    "\n",
    "# Iterate over the topics and plot word clouds in subplots\n",
    "for i, topic in enumerate(topics):\n",
    "    # Concatenate the words in the topic\n",
    "    topic_words = ' '.join(topic[1].split('*'))\n",
    "    \n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(font_path=font_path, width=400, height=200, colormap='Set2', background_color='white').generate(topic_words)\n",
    "    \n",
    "    # Plot the word cloud in the corresponding subplot\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.set_title(f'Topic {topic[0]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig(f'img/word_cloud_{search_word}.png')\n",
    "\n",
    "# Display the combined image of word clouds\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# 文書ごとのトピック分布を行列に変換\n",
    "topic_distributions = [lda_model[doc] for doc in corpus]\n",
    "topic_matrix = np.zeros((len(topic_distributions), num_topics))\n",
    "\n",
    "for i, dist in enumerate(topic_distributions):\n",
    "    for topic_id, prob in dist:\n",
    "        topic_matrix[i, topic_id] = prob\n",
    "\n",
    "# トピック間の相関行列を計算\n",
    "topic_correlation_matrix = np.corrcoef(topic_matrix.T)\n",
    "\n",
    "# DataFrameに変換して可視化\n",
    "topic_correlation_df = pd.DataFrame(topic_correlation_matrix)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(topic_correlation_df, annot=True, cmap=\"coolwarm\")\n",
    "plt.title(\"Topic Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドキュメントごとのトピック分布を取得\n",
    "doc_topic_dist = lda_model.get_document_topics(corpus, minimum_probability=0)\n",
    "\n",
    "# トピック分布をデータフレームに変換\n",
    "#doc_topic_dist = pd.DataFrame(doc_topic_dist)\n",
    "\n",
    "doc_topic_dist = [[topic_prob[1] for topic_prob in doc] for doc in doc_topic_dist]\n",
    "doc_topic_dist = pd.DataFrame(doc_topic_dist)\n",
    "\n",
    "# 文書全体におけるトピック分布の合計値を計算\n",
    "topic_dist_sum =doc_topic_dist.sum(axis=0)\n",
    "\n",
    "# 可視化\n",
    "topic_dist_sum.plot(kind='bar', figsize=(10, 6), color='skyblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トピックの時系列変化を可視化\n",
    "# 時系列データとして扱うために、日付をインデックスに設定\n",
    "data['create_at'] = pd.to_datetime(data['create_at'])\n",
    "\n",
    "# トピック分布をデータフレームに追加\n",
    "data = pd.concat([data, doc_topic_dist], axis=1)\n",
    "\n",
    "# トピック分布の時系列変化を可視化\n",
    "data.set_index('create_at').drop(columns=['key', 'tokenized_body']).plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
