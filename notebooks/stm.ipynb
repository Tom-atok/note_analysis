{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if 'root_dir' not in globals():\n",
    "    # rootディレクトリへのパスを設定\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "    os.chdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#データベースへ接続するエンジンを作成\n",
    "from my_codes.database_setting import Engine\n",
    "from my_codes.database_setting import Base\n",
    "\n",
    "#データベースのテーブルとマッピングする\n",
    "from my_codes.notes_database import Notes\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import func\n",
    "\n",
    "#セッションを作成\n",
    "Session = sessionmaker(bind=Engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 検索語を指定して，特定の用語を含むノートを検索\n",
    "search_word = '料理'\n",
    "num_samples = 10000\n",
    "result = session.query(Notes.key, Notes.urlname, Notes.created_at,Notes.tokenized_body).filter(Notes.tokenized_body.like(f'%{search_word}%')).order_by(func.random()).limit(num_samples).all()\n",
    "session.close()\n",
    "\n",
    "data = pd.DataFrame(result ,columns=['key','urlname', 'created_at', 'tokenized_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.created_at.describe()\n",
    "# data.created_atを月単位に変更する\n",
    "data['created_at'] = pd.to_datetime(data['created_at'])\n",
    "data['created_at'] = data['created_at'].dt.to_period('M')\n",
    "data.created_at.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 月毎のノート数をカウント\n",
    "data.created_at.value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.urlname.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects.numpy2ri\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RとPythonのデータフレームの相互変換を有効化\n",
    "pandas2ri.activate()\n",
    "rpy2.robjects.numpy2ri.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.created_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 語彙リストを作成\n",
    "vocab = list(set([word for doc in data['tokenized_body'] for word in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各文書をインデックスと頻度のリストに変換\n",
    "# 各文書をインデックスと頻度のリストに変換\n",
    "documents = []\n",
    "total_doc_n = len(data['tokenized_body'])\n",
    "for i, doc in enumerate(data['tokenized_body'], start=1):\n",
    "    word_counts = Counter(doc)\n",
    "    indices = [vocab.index(word) for word in word_counts.keys()]\n",
    "    counts = list(word_counts.values())\n",
    "    documents.append(list(zip(indices, counts)))\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Processed {i} out of {total_doc_n} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rのリスト形式に変換\n",
    "documents_r = ro.ListVector([(i, ro.IntVector([item[0] for item in doc])) for i, doc in enumerate(documents)])\n",
    "counts_r = ro.ListVector([(i, ro.IntVector([item[1] for item in doc])) for i, doc in enumerate(documents)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RのSTMパッケージをインポート\n",
    "stm = importr('stm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの変換\n",
    "metadata = pandas2ri.py2rpy(data[['created_at']].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トピックモデルの作成\n",
    "model = stm.stm(documents=documents_r, vocab=vocab, K=3, prevalence=metadata, data=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# トピックの表示\n",
    "ro.r('print')(model)\n",
    "\n",
    "# トピック分布の可視化\n",
    "def plot_topics(model):\n",
    "    topics = ro.r('summary')(model)\n",
    "    num_topics = len(topics.names)\n",
    "    for i in range(num_topics):\n",
    "        topic_words = topics[i]\n",
    "        plt.figure()\n",
    "        plt.barh(range(len(topic_words)), topic_words, align='center')\n",
    "        plt.yticks(range(len(topic_words)), topic_words.names)\n",
    "        plt.title(f'Topic {i + 1}')\n",
    "        plt.show()\n",
    "\n",
    "plot_topics(model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
