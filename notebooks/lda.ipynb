{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if 'root_dir' not in globals():\n",
    "    # rootディレクトリへのパスを設定\n",
    "    root_dir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "    os.chdir(root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#データベースへ接続するエンジンを作成\n",
    "from my_codes.database_setting import Engine\n",
    "from my_codes.database_setting import Base\n",
    "\n",
    "#データベースのテーブルとマッピングする\n",
    "from my_codes.notes_database import Notes\n",
    "\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy import func\n",
    "\n",
    "#セッションを作成\n",
    "Session = sessionmaker(bind=Engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORMを使って特定のカラムを取得\n",
    "num_samples = 10000  # サンプリングするデータの数\n",
    "result = session.query(Notes.key, Notes.tokenized_body).order_by(func.random()).limit(num_samples).all()\n",
    "session.close()\n",
    "# リストをDataFrameに変換\n",
    "data = pd.DataFrame(result, columns=['key','tokenized_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの形を確認\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data.tokenized_body[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast  # For converting string representation of lists to actual lists\n",
    "\n",
    "# Convert the string representation of lists in 'tokenized_body' to actual lists\n",
    "data['tokenized_body'] = data['tokenized_body'].apply(ast.literal_eval)\n",
    "\n",
    "# Display the transformed data to ensure correct conversion\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data['tokenized_body'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "# !pip install scipy==1.12\n",
    "# 最新のscipyのバージョンだとtriuがうまくダウンロードできないので、バージョンを指定してインストールする\n",
    "\n",
    "# Prepare the list of tokens for gensim\n",
    "texts = data['tokenized_body'].tolist()\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "# Convert document into the bag-of-words (BoW) format = list of (token_id, token_count)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Set up the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=20, id2word=dictionary, passes=10)\n",
    "\n",
    "# Show the topics with their terms\n",
    "topics = lda_model.print_topics(num_words=10)\n",
    "topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 日本語フォントのパスを指定\n",
    "font_path = '/Library/Fonts/Arial Unicode.ttf'\n",
    "\n",
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(20, 16))\n",
    "\n",
    "# Iterate over the topics and plot word clouds in subplots\n",
    "for i, topic in enumerate(topics):\n",
    "    # Concatenate the words in the topic\n",
    "    topic_words = ' '.join(topic[1].split('*'))\n",
    "    \n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(font_path=font_path, width=400, height=200, colormap='Set2', background_color='white').generate(topic_words)\n",
    "    \n",
    "    # Plot the word cloud in the corresponding subplot\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.set_title(f'Topic {topic[0]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the combined image of word clouds\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 各トピックのトピック分布ベクトルを取得\n",
    "topic_distributions = lda_model.get_topics()\n",
    "\n",
    "# トピック分布をコーパス形式に変換\n",
    "corpus_topic_distributions = [list(enumerate(topic)) for topic in topic_distributions]\n",
    "\n",
    "# 類似度マトリックスの初期化\n",
    "similarity_matrix = np.zeros((len(topic_distributions), len(topic_distributions)))\n",
    "\n",
    "# トピック間の類似度計算\n",
    "index = similarities.MatrixSimilarity(corpus_topic_distributions, num_features=topic_distributions.shape[1])\n",
    "for i, topic_dist in enumerate(corpus_topic_distributions):\n",
    "    sims = index[topic_dist]\n",
    "    similarity_matrix[i] = sims\n",
    "\n",
    "# 類似度マトリックスを表示\n",
    "print(similarity_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ヒートマップを作成して表示\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(similarity_matrix, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar(label='Similarity')\n",
    "plt.title('Topic Similarity Matrix')\n",
    "plt.xlabel('Topic')\n",
    "plt.ylabel('Topic')\n",
    "plt.xticks(range(len(topics)), [f'Topic {i}' for i in range(len(topics))], rotation=90)\n",
    "plt.yticks(range(len(topics)), [f'Topic {i}' for i in range(len(topics))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドキュメントごとのトピック分布を取得\n",
    "doc_topic_dist = lda_model.get_document_topics(corpus, minimum_probability=0)\n",
    "\n",
    "# トピック分布をデータフレームに変換\n",
    "doc_topic_dist = pd.DataFrame(doc_topic_dist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トピック分布の確認\n",
    "doc_topic_dist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トピックの割合を取得\n",
    "doc_topic_dist['topic'] = doc_topic_dist.idxmax(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
